{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24f470f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# COMPLETE SAGEMAKER WORKFLOW FOR IMDB SENTIMENT\n",
    "# ===========================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import json\n",
    "import boto3\n",
    "import pickle\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.utils import shuffle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.xgboost import XGBoost, XGBoostPredictor\n",
    "\n",
    "# Initialize SageMaker\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = session.boto_region_name\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'imdb-sentiment-analysis'\n",
    "\n",
    "# ===========================================\n",
    "# 1. DATA PREPARATION\n",
    "# ===========================================\n",
    "\n",
    "def download_imdb_data():\n",
    "    \"\"\"Download IMDb dataset if not already present\"\"\"\n",
    "    data_dir = Path('../data/aclImdb')\n",
    "    if not data_dir.exists():\n",
    "        print(\"Downloading IMDb dataset...\")\n",
    "        # In production, we'd use a more reliable source\n",
    "        os.system('wget -P ../data/ https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')\n",
    "        os.system('tar -xvzf ../data/aclImdb_v1.tar.gz -C ../data/')\n",
    "    return str(data_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1147915",
   "metadata": {},
   "source": [
    "# ===========================================\n",
    "# 2. PROCESSING PIPELINE\n",
    "# ===========================================\n",
    "\n",
    "def create_processing_step():\n",
    "    \"\"\"Create SageMaker Processing Job\"\"\"\n",
    "    # Use optimized sklearn container\n",
    "    sklearn_processor = SKLearnProcessor(\n",
    "        framework_version='1.2-1',\n",
    "        role=role,\n",
    "        instance_type='ml.m5.large',\n",
    "        instance_count=1,\n",
    "        base_job_name='imdb-processing'\n",
    "    )\n",
    "    \n",
    "    return ProcessingStep(\n",
    "        name='IMDBDataProcessing',\n",
    "        processor=sklearn_processor,\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=download_imdb_data(),\n",
    "                destination='/opt/ml/processing/input/data'\n",
    "            )\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name='train',\n",
    "                source='/opt/ml/processing/output/train',\n",
    "                destination=f's3://{bucket}/{prefix}/processed/train'\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name='validation',\n",
    "                source='/opt/ml/processing/output/validation',\n",
    "                destination=f's3://{bucket}/{prefix}/processed/validation'\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name='test',\n",
    "                source='/opt/ml/processing/output/test',\n",
    "                destination=f's3://{bucket}/{prefix}/processed/test'\n",
    "            )\n",
    "        ],\n",
    "        code='processing_script.py'  # Contains our processing logic\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c2fe86",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 3. MODEL TRAINING\n",
    "# ===========================================\n",
    "\n",
    "def create_training_step(train_s3, val_s3):\n",
    "    \"\"\"Create SageMaker Training Job\"\"\"\n",
    "    # Get XGBoost container\n",
    "    container = sagemaker.image_uris.retrieve('xgboost', region, '1.7-1')\n",
    "    \n",
    "    # Hyperparameters with validation\n",
    "    hyperparameters = {\n",
    "        'max_depth': '5',\n",
    "        'eta': '0.2',\n",
    "        'gamma': '4',\n",
    "        'min_child_weight': '6',\n",
    "        'subsample': '0.8',\n",
    "        'objective': 'binary:logistic',\n",
    "        'early_stopping_rounds': '10',\n",
    "        'num_round': '100'\n",
    "    }\n",
    "    \n",
    "    # Configure estimator\n",
    "    xgb_estimator = XGBoost(\n",
    "        entry_point='train.py',\n",
    "        hyperparameters=hyperparameters,\n",
    "        image_uri=container,\n",
    "        role=role,\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m5.2xlarge',\n",
    "        framework_version='1.7-1',\n",
    "        output_path=f's3://{bucket}/{prefix}/models',\n",
    "        use_spot_instances=True,\n",
    "        max_wait=7200,\n",
    "        max_run=3600\n",
    "    )\n",
    "    \n",
    "    return TrainingStep(\n",
    "        name='IMDBModelTraining',\n",
    "        estimator=xgb_estimator,\n",
    "        inputs={\n",
    "            'train': TrainingInput(s3_data=train_s3, content_type='text/csv'),\n",
    "            'validation': TrainingInput(s3_data=val_s3, content_type='text/csv')\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f5c3fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 4. DEPLOYMENT\n",
    "# ===========================================\n",
    "\n",
    "def deploy_model(model_data, endpoint_name):\n",
    "    \"\"\"Deploy trained model to endpoint\"\"\"\n",
    "    model = sagemaker.model.Model(\n",
    "        image_uri=sagemaker.image_uris.retrieve('xgboost', region, '1.7-1'),\n",
    "        model_data=model_data,\n",
    "        role=role,\n",
    "        predictor_cls=XGBoostPredictor\n",
    "    )\n",
    "    \n",
    "    # Production deployment configuration\n",
    "    predictor = model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type='ml.m5.large',\n",
    "        endpoint_name=endpoint_name,\n",
    "        data_capture_config=sagemaker.model_monitor.DataCaptureConfig(\n",
    "            enable_capture=True,\n",
    "            sampling_percentage=100,\n",
    "            destination_s3_uri=f's3://{bucket}/{prefix}/monitoring'\n",
    "        )\n",
    "    )\n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3283fb64",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 5. INFERENCE\n",
    "# ===========================================\n",
    "\n",
    "def run_batch_transform(test_s3, model_data):\n",
    "    \"\"\"Run batch transform on test data\"\"\"\n",
    "    transformer = sagemaker.transformer.Transformer(\n",
    "        model_data=model_data,\n",
    "        instance_count=1,\n",
    "        instance_type='ml.m5.xlarge',\n",
    "        output_path=f's3://{bucket}/{prefix}/predictions',\n",
    "        strategy='SingleRecord',\n",
    "        assemble_with='Line',\n",
    "        accept='text/csv'\n",
    "    )\n",
    "    \n",
    "    transformer.transform(\n",
    "        data=test_s3,\n",
    "        content_type='text/csv',\n",
    "        split_type='Line'\n",
    "    )\n",
    "    transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33be2d6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# 6. PIPELINE ORCHESTRATION\n",
    "# ===========================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Execute end-to-end workflow\"\"\"\n",
    "    # Create processing step\n",
    "    processing_step = create_processing_step()\n",
    "    \n",
    "    # Create training step\n",
    "    train_s3 = f's3://{bucket}/{prefix}/processed/train'\n",
    "    val_s3 = f's3://{bucket}/{prefix}/processed/validation'\n",
    "    training_step = create_training_step(train_s3, val_s3)\n",
    "    \n",
    "    # Define pipeline\n",
    "    pipeline = Pipeline(\n",
    "        name='IMDBSentimentPipeline',\n",
    "        steps=[processing_step, training_step],\n",
    "        parameters=[],\n",
    "        sagemaker_session=session\n",
    "    )\n",
    "    \n",
    "    # Execute pipeline\n",
    "    pipeline.upsert(role_arn=role)\n",
    "    execution = pipeline.start()\n",
    "    execution.wait()\n",
    "    \n",
    "    # Get model data\n",
    "    model_data = execution.steps[1].properties.ModelArtifacts.S3ModelArtifacts\n",
    "    \n",
    "    # Deploy model\n",
    "    endpoint_name = 'imdb-sentiment-endpoint'\n",
    "    deploy_model(model_data, endpoint_name)\n",
    "    \n",
    "    # Run batch transform\n",
    "    test_s3 = f's3://{bucket}/{prefix}/processed/test'\n",
    "    run_batch_transform(test_s3, model_data)\n",
    "    \n",
    "    print(\"✅ End-to-end workflow completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
